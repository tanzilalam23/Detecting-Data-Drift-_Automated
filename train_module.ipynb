{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce09cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import percentile\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca015014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbfd5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                     \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Load the corpus\n",
    "content = []\n",
    "file_names = []\n",
    "path = '/mnt/c/Users/alamm9/Desktop/text_file_train_test/train_set'\n",
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path, filename), 'r') as file:\n",
    "        content.append(file.read())\n",
    "        file_names.append(filename)\n",
    "\n",
    "# Step 2: Create a dataframe from the corpus\n",
    "df_train = pd.DataFrame({'Filename': file_names, 'Content': content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425f970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = preprocessing_module.preprocess_text(df_train, \"Content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7edcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset\n",
    "df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2452f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(df_train,df):\n",
    "    \n",
    "    model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "#     with open(\"model.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(model, f)\n",
    "    \n",
    "    # Obtain document embeddings for the training set\n",
    "    train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]]\n",
    "#     with open(\"train_embeddings(w2v).pkl\", \"wb\") as f:\n",
    "#         pickle.dump(train_embeddings, f)\n",
    "#         print(train_embeddings[0])\n",
    "#         print(len(train_embeddings))\n",
    "#         print(train_embeddings[0].shape)\n",
    "    \n",
    "    \n",
    "    # Transform the test set embeddings using the learned transformation from the training set\n",
    "    test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df[\"Content\"]]\n",
    "\n",
    "    # Calculate cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) #trian 80% and validation 20%\n",
    "    print(\"similarity_matrix\",similarity_matrix.shape)\n",
    "\n",
    "#     print(similarity_matrix[0])\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate mean similarity for each row\n",
    "    mean_similarity = np.mean(similarity_matrix, axis=0)\n",
    "#     print(mean_similarity)\n",
    "#     print(mean_similarity.shape)\n",
    "    threshold = np.percentile(mean_similarity, [10])\n",
    "    print(\"threshold(w2v):\", threshold)\n",
    "#     with open(\"threshold(w2v).pkl\", \"wb\") as f:\n",
    "#         pickle.dump(threshold, f)\n",
    "\n",
    "\n",
    "#************************************************************************************************************************\n",
    "\n",
    "def tfidf(df_train,df):\n",
    "    \n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Fitting vectorizer in the training dataset\n",
    "    train = vectorizer.fit_transform(df_train[\"Content\"])\n",
    "\n",
    "    # with open(\"train(tfidf).pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(train, f)\n",
    "    #    print(\"train(tfidf):\", train)\n",
    "\n",
    "    \n",
    "    # with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(vectorizer, f)\n",
    "    #    print(\"vectorizer(tfidf):\", vectorizer)\n",
    "\n",
    "\n",
    "    # Apply the same vectorizer on the test documents\n",
    "    test = vectorizer.transform(df[\"Content\"])\n",
    "\n",
    "    similarity_matrix1 = cosine_similarity(train, test) #trian 80% and validation 20%\n",
    "    print(\"similarity_matrix\",similarity_matrix1.shape)\n",
    "\n",
    "    # Calculate mean similarity for each row\n",
    "    mean_similarity1 = np.mean(similarity_matrix1, axis=0)\n",
    "    # print(mean_similarity1)\n",
    "    # print(mean_similarity1.shape)\n",
    "    threshold_tfidf = np.percentile(mean_similarity1, 10)\n",
    "    print(\"threshold(tfidf):\", threshold_tfidf)\n",
    "    # with open(\"threshold(tfidf).pkl\", \"wb\") as f:\n",
    "    #      pickle.dump(threshold_tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb074c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf(df_train,df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92af385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v(df_train,df_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
