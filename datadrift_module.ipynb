{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e826f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import percentile\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156da251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6423bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_drift(unseen_report, tfidf=bool):\n",
    "    \n",
    "    # Create a dataframe for the unseen report using the create_dataframe function\n",
    "    \n",
    "    content = []\n",
    "    file_names = []\n",
    "    for filename in os.listdir(unseen_report):\n",
    "        with open(os.path.join(unseen_report, filename), 'r') as file:\n",
    "            content.append(file.read())\n",
    "            file_names.append(filename)\n",
    "\n",
    "    df_unseen = pd.DataFrame({'Filename': file_names, 'Content': content})\n",
    "\n",
    "    # Preprocess the text in the dataframe using the preprocess_text function\n",
    "    df_unseen = preprocessing_module.preprocess_text(df_unseen, 'Content')\n",
    "    \n",
    "      \n",
    "    if (tfidf==1):\n",
    "        \n",
    "        print(\"This is TF-IDF Calculation\")\n",
    "        \n",
    "        # Initialize the TF-IDF vectorizer\n",
    "        with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "\n",
    "        # Apply the same vectorizer on the test documents\n",
    "        test = vectorizer.transform(df_unseen[\"Content\"])\n",
    "        \n",
    "        # Initialize train vectorizer\n",
    "        with open(\"train(tfidf).pkl\", \"rb\") as f:\n",
    "            train = pickle.load(f)\n",
    "\n",
    "        # Initialize threshold vectorizer\n",
    "        with open(\"threshold(tfidf).pkl\", \"rb\") as f:\n",
    "            threshold = pickle.load(f)\n",
    "            \n",
    "        # Calculate Cosine Similarity\n",
    "        similarity_matrix = cosine_similarity(train, test) \n",
    "\n",
    "        # Calculate mean similarity for each row (i.e.; via test dataset)\n",
    "        mean_similarity = np.mean(similarity_matrix, axis=0)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(\"This is Word2Vec Calculation\")\n",
    "        \n",
    "        # Initialize the train embeddings\n",
    "        with open(\"train_embeddings(w2v).pkl\", \"rb\") as f:\n",
    "            train_embeddings = pickle.load(f)\n",
    "\n",
    "        # Initialize the threshold \n",
    "        with open(\"threshold(w2v).pkl\", \"rb\") as f:\n",
    "            threshold = pickle.load(f)\n",
    "        \n",
    "        # Initialize the Word2Vec Embedding\n",
    "        with open(\"model.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        # Applying same Word2Vec Embedding on the test document\n",
    "        unseen_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_unseen[\"Content\"]]\n",
    "\n",
    "        # Calculate cosine similarity \n",
    "        similarity_matrix = cosine_similarity(train_embeddings, unseen_embeddings)\n",
    "\n",
    "        # Calculate mean similarity for each row (i.e.; via test dataset)\n",
    "        mean_similarity = np.mean(similarity_matrix, axis=0)\n",
    "        \n",
    "\n",
    "   # Determine data drift and print document information\n",
    "    messages = []\n",
    "    for doc_idx, similarity_score in enumerate(mean_similarity):\n",
    "        is_drift = similarity_score <= threshold\n",
    "        drift_status = \"ð——ð—®ð˜ð—® ð——ð—¿ð—¶ð—³ð˜\" if is_drift else \"No Data Drift\"\n",
    "        similarity_str = \"{:}\".format(similarity_score)  \n",
    "        message = f\"Document {doc_idx + 1:<4}: Mean Similarity = {similarity_str:<20} {drift_status}\"\n",
    "        messages.append(message)\n",
    "\n",
    "    result = (messages)\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
