{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d960c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a12b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                     \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Load the corpus\n",
    "content = []\n",
    "file_names = []\n",
    "path = '/mnt/c/Users/alamm9/Desktop/text_file_train_test/train_set'\n",
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path, filename), 'r') as file:\n",
    "        content.append(file.read())\n",
    "        file_names.append(filename)\n",
    "\n",
    "# Step 2: Create a dataframe from the corpus\n",
    "df_train = pd.DataFrame({'Filename': file_names, 'Content': content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da4a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "file_names = []\n",
    "path = '/mnt/c/Users/alamm9/Desktop/text_file_train_test/In_sample_test'\n",
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path, filename), 'r') as file:\n",
    "        content.append(file.read())\n",
    "        file_names.append(filename)\n",
    "\n",
    "# Step 2: Create a dataframe from the corpus\n",
    "df_test = pd.DataFrame({'Filename': file_names, 'Content': content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "file_names = []\n",
    "path = '/mnt/c/Users/alamm9/Desktop/text_file_train_test/Out_of_sample_test'\n",
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path, filename), 'r') as file:\n",
    "        content.append(file.read())\n",
    "        file_names.append(filename)\n",
    "\n",
    "# Step 2: Create a dataframe from the corpus\n",
    "df_test_out = pd.DataFrame({'Filename': file_names, 'Content': content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10c8e4f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_foreign\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_dataframe_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m, in \u001b[0;36mcreate_dataframe_from_files\u001b[0;34m(unseen_report)\u001b[0m\n\u001b[1;32m      2\u001b[0m content \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m file_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43munseen_report\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      6\u001b[0m     full_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(unseen_report, filename)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Check if the file is a CSV\u001b[39;00m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake.csv'"
     ]
    }
   ],
   "source": [
    "df_foreign=create_dataframe_from_files(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c424e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foreign = pd.read_csv(\"/mnt/c/Users/alamm9/Desktop/text_file_train_test/fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b8daab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df, column):\n",
    "    \n",
    "\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    df[column] = df[column].apply(lambda text: re.sub(r'[^a-zA-Z\\s]', '', str(text)))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    df[column] = df[column].str.lower()\n",
    "    \n",
    "    # Remove multiple occurrences of 'i' characters\n",
    "    df[column] = df[column].apply(lambda text: re.sub(r'(i{2,})', 'i', str(text)))\n",
    "    \n",
    "    # Remove single alphabets\n",
    "    df[column] = df[column].apply(lambda text: re.sub(r'\\b[a-zA-Z]\\b', '', str(text)))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    df[column] = df[column].apply(lambda text: nltk.word_tokenize(text))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df[column] = df[column].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[column] = df[column].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "    \n",
    "    # Remove 'mmddyyy'\n",
    "    df[column] = df[column].apply(lambda text: re.sub(r'(\\b[mmddyyy]+\\b)', '', str(text)))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    df[column] = df[column].apply(lambda text: re.sub(r'\\s+', '', str(text)))\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    df[column] = df[column].apply(lambda tokens: ''.join(tokens))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "799d1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = preprocess_text(df_train, \"Content\")\n",
    "#df_test = preprocess_text(df_test, \"Content\")\n",
    "#df_foreign = preprocess_text(df_foreign, \"text\")\n",
    "#df_test_out = preprocess_text(df_test_out, \"Content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[\"Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "331f60d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset\n",
    "df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f25d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4a8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "121b8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import percentile\n",
    "import os  \n",
    "\n",
    "import joblib\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "def w2v(df_train,df, tfidf=bool):# , percentile_value\n",
    "    \n",
    "    \n",
    "    if (tfidf==0):\n",
    "        # Train Word2Vec model\n",
    "        model = gensim.models.Word2Vec(df_train[\"Content\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "        # Obtain document embeddings for the training set\n",
    "        train_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df_train[\"Content\"]]\n",
    "\n",
    "        # Transform the test set embeddings using the learned transformation from the training set\n",
    "        test_embeddings = [np.mean([model.wv[token] for token in doc_tokens if token in model.wv], axis=0) for doc_tokens in df[\"Content\"]]\n",
    "\n",
    "        # Calculate cosine similarity matrix\n",
    "        similarity_matrix = cosine_similarity(train_embeddings, test_embeddings) #trian 80% and validation 20%\n",
    "\n",
    "\n",
    "        # Calculate mean similarity for each row\n",
    "        mean_similarity = np.mean(similarity_matrix, axis=0)\n",
    "        threshold = np.percentile(mean_similarity, [10])\n",
    "    \n",
    "    else:\n",
    "        # Initialize the TF-IDF vectorizer\n",
    "        with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "\n",
    "        # Apply the same vectorizer on the test documents\n",
    "        test = vectorizer.transform(df_unseen[\"Content\"])\n",
    "        \n",
    "\n",
    "        with open(\"train(tfidf).pkl\", \"rb\") as f:\n",
    "            train = pickle.load(f)\n",
    "\n",
    "        with open(\"threshold(tfidf).pkl\", \"rb\") as f:\n",
    "            threshold = pickle.load(f)\n",
    "\n",
    "        similarity_matrix = cosine_similarity(train, test) #trian 80% and validation 20%\n",
    "        print(\"similarity_matrix\",similarity_matrix.shape)\n",
    "\n",
    "        # Calculate mean similarity for each row\n",
    "        mean_similarity = np.mean(similarity_matrix, axis=0)\n",
    "        threshold_tfidf = np.percentile(mean_similarity, 10)\n",
    "        #print(\"threshold(tfidf):\", threshold_tfidf)\n",
    "\n",
    "    \n",
    "    \n",
    "#   # Determine data drift and print document information\n",
    "    messages = []\n",
    "    for doc_idx, similarity_score in enumerate(mean_similarity):\n",
    "        is_drift = similarity_score <= threshold\n",
    "        drift_status = \"ð——ð—®ð˜ð—® ð——ð—¿ð—¶ð—³ð˜\" if is_drift else \"No Data Drift\"\n",
    "        similarity_str = \"{:}\".format(similarity_score)  \n",
    "        message = f\"Document {doc_idx + 1:<4}: Mean Similarity = {similarity_str:<20} {drift_status}\"\n",
    "        messages.append(message)\n",
    "\n",
    "    # Combine all messages into a single string\n",
    "    result = (messages)\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edee062d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_unseen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mw2v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 39\u001b[0m, in \u001b[0;36mw2v\u001b[0;34m(df_train, df, tfidf)\u001b[0m\n\u001b[1;32m     36\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Apply the same vectorizer on the test documents\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mdf_unseen\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain(tfidf).pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     43\u001b[0m     train \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_unseen' is not defined"
     ]
    }
   ],
   "source": [
    "w2v(df_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08abf98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48cc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35455faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78425489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f05bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the heatmap\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# heatmap = plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar(heatmap)\n",
    "# plt.title('Cosine Similarity Heatmap')\n",
    "# plt.xlabel('Training Set')\n",
    "# plt.ylabel('Test Set')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc65d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the histogram\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(cosine_similarity_values, bins=10, color='skyblue')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Cosine Similarity')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pickle\n",
    "# def tfidf(train, column_train):\n",
    "    \n",
    "#     # Initialize the TF-IDF vectorizer\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "    \n",
    "#     # Fit the vectorizer on the training documents\n",
    "#     train = vectorizer.fit_transform(train[column_train])\n",
    "    \n",
    "#     # Save the train variable to a file\n",
    "#     with open('train_tfidf.pkl', 'wb') as file:\n",
    "#         pickle.dump(train, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3906e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf(df_train, \"Content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a963b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4e593d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7ba16bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train(tfidf):   (0, 17822)\t0.04779348001825221\n",
      "  (0, 13367)\t0.023050959410843275\n",
      "  (0, 7940)\t0.019033249489315292\n",
      "  (0, 13705)\t0.019398573642811022\n",
      "  (0, 8237)\t0.04077177354645408\n",
      "  (0, 2131)\t0.013993877358571556\n",
      "  (0, 11598)\t0.03450286941654411\n",
      "  (0, 12676)\t0.026162377526183916\n",
      "  (0, 4524)\t0.02015607729617889\n",
      "  (0, 6922)\t0.04779348001825221\n",
      "  (0, 4320)\t0.04779348001825221\n",
      "  (0, 4070)\t0.03251024424045488\n",
      "  (0, 10599)\t0.03139977890194963\n",
      "  (0, 964)\t0.03276482338373805\n",
      "  (0, 2650)\t0.03115856413676053\n",
      "  (0, 965)\t0.02661108331853525\n",
      "  (0, 2655)\t0.02949991580409821\n",
      "  (0, 3797)\t0.03092612278773077\n",
      "  (0, 14915)\t0.02627163544600588\n",
      "  (0, 960)\t0.03246749383607051\n",
      "  (0, 3694)\t0.029685187694248976\n",
      "  (0, 3059)\t0.022601125287185542\n",
      "  (0, 12524)\t0.01596301988344955\n",
      "  (0, 8783)\t0.03582998243585233\n",
      "  (0, 5649)\t0.08849974741229462\n",
      "  :\t:\n",
      "  (1299, 9743)\t0.022674467092546315\n",
      "  (1299, 5335)\t0.05149278225145697\n",
      "  (1299, 16045)\t0.02680062644682873\n",
      "  (1299, 16590)\t0.015921156045539637\n",
      "  (1299, 2328)\t0.0738572572024966\n",
      "  (1299, 13808)\t0.04738136950955509\n",
      "  (1299, 14518)\t0.05388399432086356\n",
      "  (1299, 10370)\t0.038549537693115754\n",
      "  (1299, 6798)\t0.08075146753442396\n",
      "  (1299, 10839)\t0.05364468156486768\n",
      "  (1299, 9603)\t0.05351453869653073\n",
      "  (1299, 2032)\t0.027900112018656283\n",
      "  (1299, 2817)\t0.21879547946892428\n",
      "  (1299, 13782)\t0.050445426777719925\n",
      "  (1299, 6979)\t0.040758835921409935\n",
      "  (1299, 16467)\t0.1653116897740827\n",
      "  (1299, 2851)\t0.024107372427574315\n",
      "  (1299, 14403)\t0.03547174529925316\n",
      "  (1299, 2726)\t0.014015947456630805\n",
      "  (1299, 297)\t0.12506430327154736\n",
      "  (1299, 9818)\t0.02710682395388194\n",
      "  (1299, 3860)\t0.045897457332184394\n",
      "  (1299, 12769)\t0.055333831820809073\n",
      "  (1299, 243)\t0.06864253935711559\n",
      "  (1299, 15228)\t0.019315980344445205\n",
      "vectorizer(tfidf): TfidfVectorizer()\n",
      "similarity_matrix (1300, 326)\n",
      "threshold(tfidf): 0.051730698012810154\n"
     ]
    }
   ],
   "source": [
    " # Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "train = vectorizer.fit_transform(df_train[\"Content\"])\n",
    "\n",
    "# with open(\"train(tfidf).pkl\", \"wb\") as f:\n",
    "#     pickle.dump(train, f)\n",
    "    \n",
    "    print(\"train(tfidf):\", train)\n",
    "\n",
    "    \n",
    "# with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(vectorizer, f)\n",
    "    \n",
    "    print(\"vectorizer(tfidf):\", vectorizer)\n",
    "\n",
    "\n",
    "# Apply the same vectorizer on the test documents\n",
    "test = vectorizer.transform(df_validation[\"Content\"])\n",
    "\n",
    "similarity_matrix1 = cosine_similarity(train, test) #trian 80% and validation 20%\n",
    "print(\"similarity_matrix\",similarity_matrix1.shape)\n",
    "\n",
    "# Calculate mean similarity for each row\n",
    "mean_similarity1 = np.mean(similarity_matrix1, axis=0)\n",
    "# print(mean_similarity1)\n",
    "# print(mean_similarity1.shape)\n",
    "threshold_tfidf = np.percentile(mean_similarity1, 10)\n",
    "print(\"threshold(tfidf):\", threshold_tfidf)\n",
    "# with open(\"threshold(tfidf).pkl\", \"wb\") as f:\n",
    "#      pickle.dump(threshold_tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb016629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
